#USER pipelineYaml: "${OBS_SUBARU_DIR}/pipelines/DRP.yaml#processCcd"
#  OR
#USER qgraphFile: "/path/to/existing/file.qgraph"


# At minimum, following group used in bps report and can be
# used in WMS queries.  Users can use in other things such as "output".
#USER operator: special_name    # defaults to login on submit machine
#USER project: dev
#USER campaign: quick

# Any extra site-specific settings needed for WMS
#USER computeSite: ncsapool

# Options that go between executable and command (e.g., between pipetask and run)
defaultPreCmdOpts: "--long-log --log-level=VERBOSE"
# for createQuantumGraph
qgraphPreCmdOpts: "{defaultPreCmdOpts}"
# for pipetaskInit
initPreCmdOpts: "{defaultPreCmdOpts}"
# for running Quanta
runPreCmdOpts: "{defaultPreCmdOpts}"

# Values defining input dataset as well as collection names of output
payload:
  #USER payloadName: pipelines_check    # Used in bps report, and default output collection
  #USER butlerConfig: ${PIPELINES_CHECK_DIR}/DATA_REPO/butler.yaml
  #USER inCollection: HSC/calib,HSC/raw/all,refcats
  #USER dataQuery: exposure=903342 AND detector=10
  runInit: true
  output: "u/{operator}/{payloadName}"
  outputRun: "{output}/{timestamp}"

# Mid-level values, rarely overridden other than by plugins or tools
pipetaskOutput: "-o {output}"
pipetaskInput: "-i {inCollection}"
pipetaskDataQuery: '-d "{dataQuery}"'
fileDistributionEndPoint: ""   # Needed by PanDA plugin, must end in / if non-empty string

pipetask:
  pipetaskInit:
    # Notes:  Declaring and chaining now happen within execution butler steps.
    # This command no longer needs -o and must have --extend-run.
    # Change panda-plugin's config at config/bps_idf.yaml too when changing runQuantumCommand
    runQuantumCommand: "${CTRL_MPEXEC_DIR}/bin/pipetask {initPreCmdOpts} run -b {butlerConfig} {pipetaskInput} {pipetaskOutput} --output-run {outputRun} --qgraph {fileDistributionEndPoint}{qgraphFile} --qgraph-id {qgraphId} --qgraph-node-id {qgraphNodeId} --clobber-outputs --init-only --extend-run {extraInitOptions}"
  #OPT myTask:
  #OPT   requestCpus:
  #OPT   requestMemory:
  #OPT   requestDisk:
  #OPT   requestWalltime:
  #OPT   runQuantumCommand:
  #OPT   memoryMultiplier:
  #OPT   numberOfRetries:

# Default commands and usage requests for creating QuantumGraph, running Quantum.
# Defaults to using full workflow QuantumGraph instead of per-job QuantumGraphs.
whenSaveJobQgraph: "NEVER"
createQuantumGraph: '${CTRL_MPEXEC_DIR}/bin/pipetask {qgraphPreCmdOpts} qgraph -b {butlerConfig} {pipetaskInput} {pipetaskOutput} --output-run {outputRun} -p {pipelineYaml} -q {qgraphFile} {pipetaskDataQuery} {extraQgraphOptions}'
runQuantumCommand: "${CTRL_MPEXEC_DIR}/bin/pipetask {runPreCmdOpts} run -b {butlerConfig} {pipetaskInput} {pipetaskOutput} --output-run {outputRun} --qgraph {fileDistributionEndPoint}{qgraphFile} --qgraph-id {qgraphId} --qgraph-node-id {qgraphNodeId} --clobber-outputs --skip-init-writes --extend-run {extraRunQuantumOptions}"
preemptible: True
requestMemory: 2048
requestCpus: 1

wmsServiceClass: lsst.ctrl.bps.htcondor.HTCondorService
bpsUseShared: True
clusterAlgorithm: lsst.ctrl.bps.quantum_clustering_funcs.single_quantum_clustering

# Templates for bps filenames
submitPath: ${PWD}/submit/{outputRun}
qgraphFileTemplate: "{uniqProcName}.qgraph"
executionButlerTemplate: "{submitPath}/EXEC_REPO-{uniqProcName}"
subDirTemplate: "{label}/{tract}/{patch}/{band}/{subfilter}/{physical_filter}/{visit}/{exposure}"
templateDataId: "{tract}_{patch}_{band}_{visit}_{exposure}_{detector}"

# Whether to output bps-specific intermediate files
saveDot: False
saveGenericWorkflow: False
saveClusteredQgraph: False

# Temporary backward-compatibility settings
useLazyCommands: True

executionButler:
  whenCreate: "SUBMIT"
  #OPT executionButlerDir: "/my/exec/butler/dir"  # User-provided or defaults to executionButlerTemplate
  createCommand: "${CTRL_MPEXEC_DIR}/bin/pipetask qgraph -b {butlerConfig} {pipetaskInput} {pipetaskOutput} --output-run {outputRun} --save-execution-butler {executionButlerDir} -g {qgraphFile}"
  whenMerge: "ALWAYS"
  implementation: JOB  # Added for future flexibility, e.g., if prefer workflow instead of shell script.
  concurrencyLimit: db_limit

  mergePreCmdOpts: "{defaultPreCmdOpts}"
  command1: "${DAF_BUTLER_DIR}/bin/butler {mergePreCmdOpts} transfer-datasets {executionButlerDir} {butlerConfig} --collections {outputRun} --register-dataset-types"
  command2: "${DAF_BUTLER_DIR}/bin/butler {mergePreCmdOpts} collection-chain {butlerConfig} {output} --flatten --mode=extend {inCollection}"
  command3: "${DAF_BUTLER_DIR}/bin/butler {mergePreCmdOpts} collection-chain {butlerConfig} {output} --flatten --mode=prepend {outputRun}"
